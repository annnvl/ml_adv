
[+] General Parameters:
Training data:	./data/train.txt
Test data:	./data/test.txt
Validation data:	./data/valid.txt
Feature vector representation: Dense.
Ranking method:	LambdaMART
Feature description file:	Unspecified. All features will be used.
Train metric:	NDCG@10
Test metric:	NDCG@10
Feature normalization: No
Model file: ./model/LambdaMART_300_5.txt

[+] LambdaMART's Parameters:
No. of trees: 300
No. of leaves: 10
No. of threshold candidates: 256
Learning rate: 0.1
Stop early: 100 rounds without performance gain on validation data

Reading feature file [./data/train.txt]: 0... Reading feature file [./data/train.txt]... [Done.]            
(938 ranked lists, 9847 entries read)
Reading feature file [./data/valid.txt]: 0... Reading feature file [./data/valid.txt]... [Done.]            
(964 ranked lists, 9726 entries read)
Reading feature file [./data/test.txt]: 0... Reading feature file [./data/test.txt]... [Done.]            
(937 ranked lists, 9619 entries read)
Initializing... [Done]
---------------------------------
Training starts...
---------------------------------
#iter   | NDCG@10-T | NDCG@10-V | 
---------------------------------
1       | 0.6762    | 0.688     | 
2       | 0.6822    | 0.6942    | 
3       | 0.6841    | 0.6953    | 
4       | 0.6827    | 0.6922    | 
5       | 0.6831    | 0.6932    | 
6       | 0.688     | 0.6935    | 
7       | 0.688     | 0.6936    | 
8       | 0.6888    | 0.6951    | 
9       | 0.6908    | 0.698     | 
10      | 0.695     | 0.7032    | 
11      | 0.6975    | 0.7058    | 
12      | 0.6994    | 0.7081    | 
13      | 0.701     | 0.7091    | 
14      | 0.7017    | 0.7079    | 
15      | 0.7025    | 0.7099    | 
16      | 0.7055    | 0.7121    | 
17      | 0.7053    | 0.7112    | 
18      | 0.7069    | 0.7133    | 
19      | 0.7073    | 0.7142    | 
20      | 0.7094    | 0.7142    | 
21      | 0.7107    | 0.7154    | 
22      | 0.7125    | 0.7145    | 
23      | 0.7139    | 0.7157    | 
24      | 0.7159    | 0.7168    | 
25      | 0.7151    | 0.7185    | 
26      | 0.7148    | 0.7197    | 
27      | 0.7173    | 0.7212    | 
28      | 0.7172    | 0.7208    | 
29      | 0.7178    | 0.721     | 
30      | 0.7187    | 0.7208    | 
31      | 0.7188    | 0.72      | 
32      | 0.7189    | 0.7205    | 
33      | 0.7201    | 0.7202    | 
34      | 0.7209    | 0.7199    | 
35      | 0.7237    | 0.7195    | 
36      | 0.7242    | 0.7191    | 
37      | 0.7252    | 0.7188    | 
38      | 0.7259    | 0.7192    | 
39      | 0.7267    | 0.7197    | 
40      | 0.7277    | 0.7191    | 
41      | 0.7291    | 0.721     | 
42      | 0.7305    | 0.7202    | 
43      | 0.7315    | 0.7213    | 
44      | 0.7334    | 0.7224    | 
45      | 0.7334    | 0.7223    | 
46      | 0.7338    | 0.7235    | 
47      | 0.7346    | 0.7228    | 
48      | 0.7362    | 0.7223    | 
49      | 0.7364    | 0.7224    | 
50      | 0.7368    | 0.7225    | 
51      | 0.7366    | 0.7222    | 
52      | 0.7373    | 0.7222    | 
53      | 0.7381    | 0.722     | 
54      | 0.739     | 0.7218    | 
55      | 0.74      | 0.7234    | 
56      | 0.7399    | 0.7242    | 
57      | 0.7402    | 0.7253    | 
58      | 0.7419    | 0.7264    | 
59      | 0.7424    | 0.726     | 
60      | 0.7422    | 0.7257    | 
61      | 0.7442    | 0.7253    | 
62      | 0.7456    | 0.7274    | 
63      | 0.7453    | 0.7277    | 
64      | 0.7452    | 0.7276    | 
65      | 0.7461    | 0.7278    | 
66      | 0.7471    | 0.7268    | 
67      | 0.7472    | 0.7272    | 
68      | 0.7495    | 0.7281    | 
69      | 0.75      | 0.7282    | 
70      | 0.7497    | 0.7281    | 
71      | 0.751     | 0.7271    | 
72      | 0.7509    | 0.7275    | 
73      | 0.7513    | 0.7279    | 
74      | 0.7516    | 0.7286    | 
75      | 0.7519    | 0.7281    | 
76      | 0.7524    | 0.7276    | 
77      | 0.7534    | 0.7285    | 
78      | 0.7532    | 0.7291    | 
79      | 0.7537    | 0.73      | 
80      | 0.7541    | 0.7298    | 
81      | 0.7551    | 0.7292    | 
82      | 0.7567    | 0.7292    | 
83      | 0.7564    | 0.7286    | 
84      | 0.7575    | 0.7273    | 
85      | 0.7583    | 0.7271    | 
86      | 0.7588    | 0.727     | 
87      | 0.759     | 0.7269    | 
88      | 0.76      | 0.7259    | 
89      | 0.7601    | 0.7258    | 
90      | 0.7607    | 0.7256    | 
91      | 0.7603    | 0.7262    | 
92      | 0.7608    | 0.726     | 
93      | 0.7617    | 0.7259    | 
94      | 0.7621    | 0.726     | 
95      | 0.7616    | 0.7268    | 
96      | 0.7619    | 0.7258    | 
97      | 0.7619    | 0.7248    | 
98      | 0.7623    | 0.7257    | 
99      | 0.7626    | 0.7257    | 
100     | 0.762     | 0.726     | 
101     | 0.7628    | 0.7262    | 
102     | 0.7634    | 0.726     | 
103     | 0.7641    | 0.7246    | 
104     | 0.7641    | 0.7246    | 
105     | 0.7641    | 0.7251    | 
106     | 0.7645    | 0.7259    | 
107     | 0.7643    | 0.7257    | 
108     | 0.7649    | 0.7254    | 
109     | 0.7654    | 0.7258    | 
110     | 0.7658    | 0.7254    | 
111     | 0.767     | 0.7258    | 
112     | 0.767     | 0.7256    | 
113     | 0.7678    | 0.7255    | 
114     | 0.7685    | 0.726     | 
115     | 0.7686    | 0.7262    | 
116     | 0.7697    | 0.7261    | 
117     | 0.7701    | 0.7257    | 
118     | 0.7705    | 0.7253    | 
119     | 0.7713    | 0.7254    | 
120     | 0.7714    | 0.7254    | 
121     | 0.7713    | 0.7258    | 
122     | 0.7727    | 0.725     | 
123     | 0.7729    | 0.7239    | 
124     | 0.7727    | 0.7245    | 
125     | 0.7732    | 0.7239    | 
126     | 0.7736    | 0.7236    | 
127     | 0.774     | 0.7239    | 
128     | 0.7742    | 0.7242    | 
129     | 0.775     | 0.7236    | 
130     | 0.7745    | 0.7234    | 
131     | 0.7747    | 0.7228    | 
132     | 0.7749    | 0.7231    | 
133     | 0.7752    | 0.7235    | 
134     | 0.7756    | 0.7237    | 
135     | 0.7765    | 0.7236    | 
136     | 0.7766    | 0.7242    | 
137     | 0.7767    | 0.7239    | 
138     | 0.7766    | 0.724     | 
139     | 0.7765    | 0.7238    | 
140     | 0.7767    | 0.7232    | 
141     | 0.7777    | 0.7239    | 
142     | 0.7776    | 0.7236    | 
143     | 0.7781    | 0.7232    | 
144     | 0.7784    | 0.7231    | 
145     | 0.7784    | 0.7235    | 
146     | 0.7785    | 0.7226    | 
147     | 0.7793    | 0.7221    | 
148     | 0.7793    | 0.7223    | 
149     | 0.7793    | 0.7227    | 
150     | 0.7795    | 0.7227    | 
151     | 0.7794    | 0.722     | 
152     | 0.7793    | 0.7234    | 
153     | 0.7797    | 0.7226    | 
154     | 0.78      | 0.7235    | 
155     | 0.7799    | 0.7239    | 
156     | 0.78      | 0.7239    | 
157     | 0.7805    | 0.7242    | 
158     | 0.7811    | 0.7244    | 
159     | 0.7811    | 0.7241    | 
160     | 0.7815    | 0.7245    | 
161     | 0.7822    | 0.7253    | 
162     | 0.7825    | 0.7243    | 
163     | 0.783     | 0.7241    | 
164     | 0.7829    | 0.7235    | 
165     | 0.7835    | 0.723     | 
166     | 0.7838    | 0.7227    | 
167     | 0.7839    | 0.7243    | 
168     | 0.7835    | 0.7245    | 
169     | 0.7834    | 0.7245    | 
170     | 0.7841    | 0.7246    | 
171     | 0.7846    | 0.7246    | 
172     | 0.7849    | 0.7245    | 
173     | 0.7852    | 0.7246    | 
174     | 0.7855    | 0.7253    | 
175     | 0.7854    | 0.7245    | 
176     | 0.7854    | 0.7247    | 
177     | 0.7853    | 0.7246    | 
178     | 0.7854    | 0.7248    | 
179     | 0.786     | 0.7255    | 
180     | 0.7857    | 0.7251    | 
---------------------------------
Finished sucessfully.
NDCG@10 on training data: 0.7537
NDCG@10 on validation data: 0.73
---------------------------------
NDCG@10 on test data: 0.7011

Model saved to: ./model/LambdaMART_300_5.txt
